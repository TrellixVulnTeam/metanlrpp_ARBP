import torch
from torch import nn
from torchmeta.modules import (MetaModule, MetaSequential)
from torchmeta.modules.utils import get_subdict
import numpy as np
from collections import OrderedDict
import math
import torch.nn.functional as F

SINE_PREMULTIPLIER = 30


class Sine(nn.Module):
    def forward(self, x):
        # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of factor 30
        return torch.sin(SINE_PREMULTIPLIER * x)


class Swish(nn.Module):
    """
    https://arxiv.org/pdf/1710.05941.pdf
    """

    def __init__(self, beta=1.0, n_dims=1, trainable=True):
        super().__init__()
        self.n_dims = n_dims
        self.trainable = trainable
        if self.trainable:
            params = torch.ones((1, 1, n_dims))
            params.uniform_(0.1, 100.0)
            self.beta = nn.Parameter(params)
        else:
            self.beta = float(beta)

    def forward(self, x):
        return x * torch.sigmoid(x * self.beta)


class SineSq(nn.Module):
    def forward(self, x):
        return torch.sin(SINE_PREMULTIPLIER * x) ** 2


class Modulo(nn.Module):
    def forward(self, x):
        return torch.fmod(SINE_PREMULTIPLIER * x, 1)


class Triangle(nn.Module):
    """ 0->1 linear up, 1->2 linear down, repeat... """

    def forward(self, x):
        up = 1 - F.relu(1 - torch.fmod(x.abs(), 2))
        down = 1 - F.relu(torch.fmod(x.abs(), 2) - 1)
        return up + down


class SineOrRelu(nn.Module):
    def forward(self, x):
        assert(len(x.shape) == 3)
        in_split = x.shape[2] // 2
        x_a = x[..., :in_split]
        x_b = x[..., in_split:]
        return torch.cat((
            torch.sin(x_a),
            F.relu(x_b)
        ), axis=-1)


class XSine(nn.Module):
    """ https: // arxiv.org / pdf / 2006.08195.pdf """

    def forward(self, x):
        return x + torch.sin(x)


class XSineSq(nn.Module):
    """ https: // arxiv.org / pdf / 2006.08195.pdf """

    def forward(self, x):
        return x + torch.sin(x)**2


class XSineSqRelu(nn.Module):
    def forward(self, x):
        return F.relu(x + torch.sin(x)**2)


class XReluSineSq(nn.Module):
    def forward(self, x):
        return F.relu(x) + torch.sin(x)**2


########################
# Initialization methods
########################

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # For PINNet, Raissi et al. 2019
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    # grab from upstream pytorch branch and paste here for now
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def init_weights_trunc_normal(m):
    # For PINNet, Raissi et al. 2019
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    if hasattr(m, 'weight'):
        fan_in = m.weight.size(1)
        fan_out = m.weight.size(0)
        std = math.sqrt(2.0 / float(fan_in + fan_out))
        mean = 0.
        # initialize with the same behavior as tf.truncated_normal
        # "The generated values follow a normal distribution with specified mean and
        # standard deviation, except that values whose magnitude is more than 2
        # standard deviations from the mean are dropped and re-picked."
        _no_grad_trunc_normal_(m.weight, mean, std, -2 * std, 2 * std)


def init_weights_normal(m):
    if hasattr(m, 'weight'):
        nn.init.kaiming_normal_(m.weight, a=0.0, nonlinearity='relu', mode='fan_in')


def init_weights_uniform(m):
    if hasattr(m, 'weight'):
        nn.init.kaiming_uniform_(m.weight, a=0.0)


def init_weights_selu(m):
    if hasattr(m, 'weight'):
        num_input = m.weight.size(-1)
        nn.init.normal_(m.weight, std=1 / math.sqrt(num_input))


def init_weights_elu(m):
    if hasattr(m, 'weight'):
        num_input = m.weight.size(-1)
        nn.init.normal_(m.weight, std=math.sqrt(1.5505188080679277) / math.sqrt(num_input))


def init_weights_xavier(m):
    if hasattr(m, 'weight'):
        nn.init.xavier_normal_(m.weight)


def sine_init(m):
    with torch.no_grad():
        if hasattr(m, 'weight'):
            num_input = m.weight.size(-1)
            # See supplement Sec. 1.5 for discussion of factor 30
            m.weight.uniform_(-np.sqrt(6 / num_input) / SINE_PREMULTIPLIER, np.sqrt(6 / num_input) / SINE_PREMULTIPLIER)


def first_layer_sine_init(m):
    with torch.no_grad():
        if hasattr(m, 'weight'):
            num_input = m.weight.size(-1)
            # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of factor 30
            m.weight.uniform_(-SINE_PREMULTIPLIER / (num_input * 30), SINE_PREMULTIPLIER / (num_input * 30))


def sdf_sphere_init_first_layer(m):
    """
    Initializes weights to produce SDF of a unit sphere.
    Custom extension of ... for posenc inputs.
    SAL: Sign Agnostic Learning of Shapes from Raw Data
    Matan Atzmon, Yaron Lipman
    https: // arxiv.org / abs / 1911.10414
    """
    if hasattr(m, 'weight'):
        d_in = m.weight.size(1)
        d_out = m.weight.size(0)
        with torch.no_grad():
            m.weight[:, :3].normal_(mean=0, std=np.sqrt(2) / np.sqrt(d_out))
            if d_in > 3:
                # Ignore PosEnc inputs.
                m.weight[:, 3:].normal_(mean=0, std=1e-5)
            m.bias.fill_(0)

        # p = 1.0
        # torch.nn.init.constant_(m.bias, 0.0)
        # torch.nn.init.normal_(m.weight, 0.0, np.sqrt(2) / np.sqrt(p * d_out))
        # # Replaces weight by dir and magnitude
        nn.utils.weight_norm(m)


def sdf_sphere_init(m):
    """
    Initializes weights to produce SDF of a unit sphere.
    SAL: Sign Agnostic Learning of Shapes from Raw Data
    Matan Atzmon, Yaron Lipman
    https: // arxiv.org / abs / 1911.10414
    """
    if hasattr(m, 'weight'):
        d_in = m.weight.size(1)
        d_out = m.weight.size(0)
        # with torch.no_grad():
        #     m.weight.normal_(mean=0, std=np.sqrt(2) / np.sqrt(d_out))
        #     m.bias.fill_(0)

        p = 1.0
        torch.nn.init.constant_(m.bias, 0.0)
        torch.nn.init.normal_(m.weight, 0.0, np.sqrt(2) / np.sqrt(p * d_out))
        # Replaces weight by dir and magnitude
        nn.utils.weight_norm(m)


def sdf_sphere_init_after_skip(m, direct_dims):
    """
    First layer after skip connection.
    Ignores the skipped content.
    """
    if hasattr(m, 'weight'):
        d_in = m.weight.size(1)
        d_out = m.weight.size(0)
        with torch.no_grad():
            # The SkipConnection outputs [skip|direct]
            # Setup direct as usual.
            m.weight[:, -direct_dims:].normal_(mean=0, std=np.sqrt(2) / np.sqrt(d_out))
            # Ignore skip inputs.
            m.weight[:, :-direct_dims].normal_(mean=0, std=1e-5)
            m.bias.fill_(0)

        nn.utils.weight_norm(m)


def sdf_sphere_init_last_layer(m):
    """
    Initializes weights to produce SDF of a unit sphere.
    SAL: Sign Agnostic Learning of Shapes from Raw Data
    Matan Atzmon, Yaron Lipman
    https: // arxiv.org / abs / 1911.10414
    """
    if hasattr(m, 'weight'):
        d_in = m.weight.size(1)
        d_out = m.weight.size(0)
        # with torch.no_grad():
        #     # Differs from paper: https://github.com/matanatz/SAL/blob/master/code/model/network.py
        #     # Multiplying mean by K makes the radius 1 / K
        #     radius = 0.5
        #     m.weight.normal_(mean=1.0 / radius * (np.sqrt(np.pi) / np.sqrt(d_in)), std=0.000001)
        #     m.bias.fill_(-1)

        p = 1
        torch.nn.init.normal_(m.weight, mean=2 * np.sqrt(np.pi) / np.sqrt(p * d_in), std=0.000001)
        torch.nn.init.constant_(m.bias, -1.0)


def flow_init_zero(m):
    """
    Sets all weights very low.
    """
    if hasattr(m, 'weight'):
        torch.nn.init.normal_(m.weight, mean=0, std=0.000001)
        torch.nn.init.constant_(m.bias, 0)


def sdf_sphere_init_full(net: nn.Module):
    """
    Initializes weights to produce SDF of a unit sphere.
    Full version handling all the specificities.
    SAL: Sign Agnostic Learning of Shapes from Raw Data
    Matan Atzmon, Yaron Lipman
    https://arxiv.org/abs/1911.10414
    """
    fc_all = [m for m in net.modules() if hasattr(m, 'weight')]

    last_skipped = -1
    skips = [m for m in net.modules() if type(m).__name__ in ['SkipConnection']]
    if len(skips) > 0:
        fc_skipped = [m for m in skips[0].inner.modules() if hasattr(m, 'weight')]
        is_skipped = [x in fc_skipped for x in fc_all]
        last_skipped = np.argwhere(is_skipped)[-1, 0]

    # First.
    sdf_sphere_init_first_layer(fc_all[0])
    for i in range(1, len(fc_all) - 1):
        # Inner.
        if i == last_skipped + 1:
            # First Layer after skipped.
            prev_layer_outdims = fc_all[i - 1].weight.size(0)
            sdf_sphere_init_after_skip(fc_all[i], direct_dims=prev_layer_outdims)
        else:
            sdf_sphere_init(fc_all[i])
    # Last.
    sdf_sphere_init_last_layer(fc_all[-1])


def get_activation(name, **kwargs):
    """
    Gets activation instance.
    """
    if name == 'swish':
        return Swish(**kwargs)
    else:
        return nls_and_inits[name][0]


# Dictionary that maps nonlinearity name to the respective function, initialization, and, if applicable,
# special first-layer initialization scheme
nls_and_inits = {
    'sine': (Sine(), sine_init, first_layer_sine_init),
    'swish': (None, init_weights_normal, None),
    'sinesq': (SineSq(), sine_init, first_layer_sine_init),
    'mod': (Modulo(), sine_init, first_layer_sine_init),
    'xsine': (XSine(), init_weights_normal, None),
    'xsinesq': (XSineSq(), init_weights_normal, None),
    'xsinesqrelu': (XSineSqRelu(), init_weights_normal, None),
    'xrelusinesq': (XReluSineSq(), init_weights_normal, None),
    'triangle': (Triangle(), init_weights_normal, None),
    'sineorrelu': (SineOrRelu(), init_weights_normal, None),
    'relu': (nn.ReLU(inplace=True), init_weights_uniform, None),
    'sigmoid': (nn.Sigmoid(), init_weights_xavier, None),
    'tanh': (nn.Tanh(), init_weights_xavier, None),
    'selu': (nn.SELU(inplace=True), init_weights_selu, None),
    'softplus': (nn.Softplus(beta=100), init_weights_normal, None),
    'elu': (nn.ELU(inplace=True), init_weights_elu, None),
}
